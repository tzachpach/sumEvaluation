{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\msc_work\\sumVeracity\\summac\\summac\n"
     ]
    }
   ],
   "source": [
    "%cd summac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summac.benchmark import SummaCBenchmark\n",
    "# import summac.run_baseline as utils_summac_benchmark\n",
    "# import random\n",
    "import transformers\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "benchmark = SummaCBenchmark(benchmark_folder=\"data/summac_benchmark/\", cut=\"test\")\n",
    "# benchmark.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 2: Main Table of Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aviv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sklearn, torch, numpy as np, json, os, tqdm, pandas as pd, nltk, seaborn as sns\n",
    "from summac.model_guardrails import NERInaccuracyPenalty\n",
    "from summac.model_summac import SummaCConv, SummaCZS\n",
    "from summac.model_baseline import BaselineScorer\n",
    "# from model_entailment import EntailmentScorer\n",
    "# from model_classifier import Classifier\n",
    "from summac.utils_scoring import ScorerWrapper\n",
    "\n",
    "use_cache = True\n",
    "scorers = [\n",
    "# \t{\"name\": \"NER\", \"model\": NERInaccuracyPenalty(), \"only_doc\": True, \"sign\": 1},\n",
    "\t#     {\"name\": \"MNLI\", \"model\": EntailmentScorer(model_card=\"roberta-large-mnli\", contradiction_idx=0), \"sign\": 1},\n",
    "\t# {\"name\": \"FactCC-CLS\", \"model\": Classifier(model_card=\"roberta-base\", score_class=1, model_file=\"/home/phillab/models/cls_roberta-base_factcc_first_0_f1_0.4766.bin\"), \"sign\": 1, \"only_doc\": True},\n",
    "# \t{\"name\": \"DAE\", \"model\": BaselineScorer(model=\"dae\"), \"only_doc\": True, \"sign\": 1},\n",
    "# \t{\"name\": \"FEQA\", \"model\": BaselineScorer(model=\"feqa\"), \"only_doc\": True, \"sign\": 1},\n",
    "# \t{\"name\": \"QuestEval\", \"model\": BaselineScorer(model=\"questeval\"), \"only_doc\": True, \"sign\": 1},\n",
    "\t{\"name\": \"SummaC-ZS-VITC-L\",\n",
    "\t \"model\": SummaCZS(granularity=\"sentence\", model_name=\"mnli-base\", imager_load_cache=use_cache), \"sign\": 1,\n",
    "\t \"only_doc\": True},\n",
    "\t# {\"name\": \"SummaC-Histo-VITC-L\", \"model\": SummaCConv(models=[\"vitc\"], granularity=\"sentence\", start_file=\"/home/phillab/models/summac/vitc_sentence_percentile_e_bacc0.744.bin\", bins=\"percentile\", imager_load_cache=use_cache, device=\"cpu\"), \"sign\": 1, \"only_doc\": True},\n",
    "\n",
    "#     {\"name\": \"SummaC-ZS-BART-L\",\n",
    "# \t \"model\": SummaCZS(granularity=\"sentence\", model_name=\"bart-mnli\", imager_load_cache=use_cache), \"sign\": 1,\n",
    "# \t \"only_doc\": True}\n",
    "]\n",
    "\n",
    "scorer_doc = ScorerWrapper(scorers, scoring_method=\"sum\", max_batch_size=20, use_caching=True)\n",
    "scorer_para = ScorerWrapper([s for s in scorers if \"only_doc\" not in s], scoring_method=\"sum\", max_batch_size=20,\n",
    "\t\t\t\t\t\t\tuse_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= xsumfaith ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]Some weights of the model checkpoint at microsoft/deberta-base-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19d056aecbf48e9ba5c250c74186f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61875cb265374796abbeedc2ed1f78a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc6ac4b43f34fd995e7d5e80cf9519e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d97b712f1941e1991d664eeb306963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.35it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.28it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.35it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.32it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.30it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.45it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.50it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.52it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:00<00:00,  2.16it/s]\u001b[A\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.40it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:21<00:31,  1.06s/it]Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  3.10it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  3.13it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.85it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.71it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.57it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.67it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.44it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.53it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.54it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.43it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:04<00:01,  1.89s/it]\u001b[A\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.15s/it]\u001b[A\n",
      "100%|██████████| 50/50 [00:35<00:00,  1.41it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.69it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 1481039.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= polytope ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  1.86it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.09it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.44it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.04it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  1.56it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:01<00:00,  2.03it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.05it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.78it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.49it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.83it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.29it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.51it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:02,  1.29it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:01<00:01,  1.20it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:04<00:01,  1.63s/it]\u001b[A\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.20s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:02,  1.47it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:01<00:01,  1.91it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.24it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.90it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.39it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.42it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.17it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.30it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:00<00:00,  1.59it/s]\u001b[A\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.56it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:24<00:36,  1.23s/it]Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.10it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.05it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  1.94it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.06it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  1.67it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:01<00:01,  1.96it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  1.74it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.70it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:02,  1.10it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:01<00:01,  1.01it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:02<00:00,  1.48it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.52it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.44it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.48it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.43it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.07it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  1.82it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:01<00:01,  1.68it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  1.86it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.63it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.35it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:01<00:01,  1.30it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  1.75it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.65it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]\u001b[A\n",
      " 50%|█████     | 2/4 [00:01<00:01,  1.23it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.45it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:00<00:01,  1.41it/s]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s]\u001b[A\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.35it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:52<00:00,  1.04s/it]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.06it/s]\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.74it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.34it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.03it/s]\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.43it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.38it/s]\n",
      "100%|██████████| 1002/1002 [00:00<00:00, 1004227.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= factcc ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.98it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:01,  1.96it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.07it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.19it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.22it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.45it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.68it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.75it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.57it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.25it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.13it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.27it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  3.14it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.63it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.35it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.46it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.33it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.38it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.61it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.56it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.94it/s]\u001b[A\n",
      " 40%|████      | 20/50 [00:13<00:19,  1.52it/s]Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  3.23it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  3.36it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.75it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.79it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.41it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.53it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.57it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.30it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  1.91it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.35it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.71it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.56it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  3.45it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  3.09it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "from summac.utils_scorer import compute_paragraph_level\n",
    "from summac.run_baseline import compute_doc_level\n",
    "\n",
    "results = []\n",
    "for dataset in benchmark.datasets:\n",
    "\tprint(\"======= %s ========\" % (dataset[\"name\"]))\n",
    "\tdatas = dataset[\"dataset\"][:50]\n",
    "\tcompute_doc_level(datas, scorer_doc)\n",
    "\tcompute_paragraph_level(scorer_para, datas)\n",
    "\n",
    "\tlabels = [d[\"label\"] for d in datas]\n",
    "\tpred_labels = [k for k in datas[0].keys() if \"pred_\" in k]\n",
    "\tfor pred_label in pred_labels:\n",
    "\t\tmodel_name, input_type = pred_label.replace(\"pred_\", \"\").split(\"|\")\n",
    "\t\tpreds = [d[pred_label] for d in datas]\n",
    "\t\tscores = [d[pred_label.replace(\"pred_\", \"\")] for d in datas]\n",
    "\t\tbalanced_acc = sklearn.metrics.balanced_accuracy_score(labels, preds)\n",
    "\t\troc_auc = sklearn.metrics.roc_auc_score(labels, scores)\n",
    "\n",
    "\t\tresults.append({\"model_name\": model_name, \"dataset_name\": dataset[\"name\"],\n",
    "\t\t\t\t\t\t\"input\": input_type, \"%s_bacc\" % (dataset[\"name\"]): balanced_acc,\n",
    "\t\t\t\t\t\t\"%s_roc_auc\" % (dataset[\"name\"]): roc_auc,\n",
    "\t\t\t\t\t\t\"labels\": labels, \"preds\": preds, \"scores\": scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed for caching\n",
    "# for scorer in scorers:\n",
    "# \tif \"SummaC\" in scorer[\"name\"]:\n",
    "# \t\tscorer[\"model\"].save_imager_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "\n",
    "\n",
    "def highlight_max(data):\n",
    "\tis_max = data == data.max()\n",
    "\treturn ['font-weight: bold' if v else '' for v in is_max]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df = df.groupby([\"model_name\", \"input\"]).agg({\"%s_bacc\" % (d): \"mean\" for d in [\"xsumfaith\", \"polytope\", \"factcc\", \"summeval\", \"frank\"]})\n",
    "df.rename(columns={k: k.replace(\"_bacc\", \"\") for k in df.keys()}, inplace=True)\n",
    "df.drop(\"total\", inplace=True)\n",
    "# df[\"overall\"] = (df[\"factcc\"] + df[\"frank\"] + df[\"polytope\"] + df[\"cogensumm\"] + df[\"summeval\"] + df[\"xsumfaith\"]) / (\n",
    "# \t6.0)\n",
    "\n",
    "df[\"overall\"] = (df[\"factcc\"] + df[\"frank\"] + df[\"polytope\"] + df[\"summeval\"] + df[\"xsumfaith\"]) / (\n",
    "\t5.0)\n",
    "\n",
    "df.style.apply(highlight_max).background_gradient(cmap=cm, high=1.0, low=0.0).set_precision(3).set_caption(\n",
    "\t\"Balanced Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # Analysis with confidence interval\n",
    "# strongest_baseline = {\"cogensumm\": \"DAE\", \"xsumfaith\": \"NER\", \"polytope\": \"QuestEval\", \"factcc\": \"DAE\",\n",
    "# \t\t\t\t\t  \"summeval\": \"QuestEval\", \"frank\": \"QuestEval\"}\n",
    "\n",
    "# P5 = 5 / 2  # Correction due to the fact that we are running 2 tests with the same data\n",
    "# P1 = 1 / 2  # Correction due to the fact that we are running 2 tests with the same data\n",
    "\n",
    "\n",
    "# def resample_balanced_acc(preds, labels, n_samples=100, sample_ratio=0.7):\n",
    "# \tN = len(preds)\n",
    "# \tidxs = list(range(N))\n",
    "# \tN_batch = int(sample_ratio * N)\n",
    "\n",
    "# \tbal_accs = []\n",
    "# \tfor _ in range(n_samples):\n",
    "# \t\trandom.shuffle(idxs)\n",
    "# \t\tbatch_preds = [preds[i] for i in idxs[:N_batch]]\n",
    "# \t\tbatch_labels = [labels[i] for i in idxs[:N_batch]]\n",
    "\n",
    "# \t\tbal_accs.append(sklearn.metrics.balanced_accuracy_score(batch_labels, batch_preds))\n",
    "# \treturn bal_accs\n",
    "\n",
    "\n",
    "# print(\"DATASET NAME\".ljust(15), \"MODEL NAME\".ljust(20))\n",
    "\n",
    "# sampled_batch_preds = {res[\"model_name\"]: [] for res in results}\n",
    "# for res in results:\n",
    "# \tif res[\"model_name\"] == \"total\":\n",
    "# \t\tprint(\"==================================================\")\n",
    "# \t\tcontinue\n",
    "\n",
    "# \tsamples = resample_balanced_acc(res[\"preds\"], res[\"labels\"])\n",
    "# \tsampled_batch_preds[res[\"model_name\"]].append(samples)\n",
    "# \tlow5, high5 = np.percentile(samples, P5), np.percentile(samples, 100 - P5)\n",
    "# \tlow1, high1 = np.percentile(samples, P1), np.percentile(samples, 100 - P1)\n",
    "# \tbacc = sklearn.metrics.balanced_accuracy_score(res[\"labels\"], res[\"preds\"])\n",
    "# \tif \"SummaC\" in res[\"model_name\"] or res[\"model_name\"] == strongest_baseline[res[\"dataset_name\"]]:\n",
    "\n",
    "# \t\tprint(res[\"dataset_name\"].ljust(15), res[\"model_name\"].ljust(20),\n",
    "# \t\t\t  \" - %.3f (%.3f - %.3f) (%.3f - %.3f)\" % (bacc, low5, high5, low1, high1))\n",
    "# \t\tif res[\"model_name\"] == strongest_baseline[res[\"dataset_name\"]]:\n",
    "# \t\t\tbl5, bh5, bl1, bh1 = low5, high5, low1, high1\n",
    "# \t\t\tprint(\"--------------\")\n",
    "# \t\telse:\n",
    "# \t\t\tif low5 >= bh5:\n",
    "# \t\t\t\tprint(\"Significant difference (p < 0.05)\")\n",
    "# \t\t\tif low1 >= bh1:\n",
    "# \t\t\t\tprint(\"Significant difference (p < 0.01)\")\n",
    "\n",
    "# print(\"==========================\")\n",
    "# print(\"==========================\")\n",
    "# print(\"==========================\")\n",
    "\n",
    "# # baseline = np.mean(np.array(sampled_batch_preds[\"QuestEval\"]), axis=0)\n",
    "# summaczs = np.mean(np.array(sampled_batch_preds[\"SummaC-ZS-VITC-L\"]), axis=0)\n",
    "# summacconv = np.mean(np.array(sampled_batch_preds[\"SummaC-Histo-VITC-L\"]), axis=0)\n",
    "\n",
    "# for model in [\"QuestEval\", \"SummaC-ZS-VITC-L\", \"SummaC-Histo-VITC-L\"]:\n",
    "# \tsamples = np.mean(np.array(sampled_batch_preds[model]), axis=0)\n",
    "# \tlow5, high5 = np.percentile(samples, P5), np.percentile(samples, 100 - P5)\n",
    "# \tlow1, high1 = np.percentile(samples, P1), np.percentile(samples, 100 - P1)\n",
    "\n",
    "# \tprint(\"OVERALL\".ljust(15), model.ljust(20), \" - (%.3f - %.3f) (%.3f - %.3f)\" % (low5, high5, low1, high1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df = df.groupby([\"model_name\", \"input\"]).agg({\"%s_roc_auc\" % (d): \"mean\" for d in [\"xsumfaith\", \"polytope\", \"factcc\", \"summeval\", \"frank\"]})\n",
    "df.rename(columns={k: k.replace(\"_roc_auc\", \"\") for k in df.keys()}, inplace=True)\n",
    "df.drop(\"total\", inplace=True)\n",
    "# df[\"overall\"] = (df[\"factcc\"] + df[\"frank\"] + df[\"polytope\"] + df[\"cogensumm\"] + df[\"summeval\"] + df[\"xsumfaith\"]) / (\n",
    "# \t6.0)\n",
    "\n",
    "df[\"overall\"] = (df[\"factcc\"] + df[\"frank\"] + df[\"polytope\"] + df[\"summeval\"] + df[\"xsumfaith\"]) / (\n",
    "\t5.0)\n",
    "\n",
    "df.style.apply(highlight_max).background_gradient(cmap=cm, high=1.0, low=0.0).set_precision(3).set_caption(\"ROC AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analysis with confidence interval\n",
    "# strongest_baseline = {\"cogensumm\": \"DAE\", \"xsumfaith\": \"QuestEval\", \"polytope\": \"QuestEval\", \"factcc\": \"DAE\",\n",
    "# \t\t\t\t\t  \"summeval\": \"QuestEval\", \"frank\": \"QuestEval\"}\n",
    "\n",
    "# P5 = 5 / 2  # Correction due to the fact that we are running 2 tests with the same data\n",
    "# P1 = 1 / 2  # Correction due to the fact that we are running 2 tests with the same data\n",
    "\n",
    "\n",
    "# def resample_roc_auc(scores, labels, n_samples=100, sample_ratio=0.7):\n",
    "# \tN = len(scores)\n",
    "# \tidxs = list(range(N))\n",
    "# \tN_batch = int(sample_ratio * N)\n",
    "\n",
    "# \troc_aucs = []\n",
    "# \tfor _ in range(n_samples):\n",
    "# \t\trandom.shuffle(idxs)\n",
    "# \t\tbatch_scores = [scores[i] for i in idxs[:N_batch]]\n",
    "# \t\tbatch_labels = [labels[i] for i in idxs[:N_batch]]\n",
    "# \t\troc_aucs.append(sklearn.metrics.roc_auc_score(batch_labels, batch_scores))\n",
    "# \treturn roc_aucs\n",
    "\n",
    "\n",
    "# sampled_batch_preds = {res[\"model_name\"]: [] for res in results}\n",
    "# print(\"DATASET NAME\".ljust(15), \"MODEL NAME\".ljust(20))\n",
    "# for res in results:\n",
    "# \tif res[\"model_name\"] == \"total\":\n",
    "# \t\tprint(\"==================================================\")\n",
    "# \t\tcontinue\n",
    "# \tsamples = resample_roc_auc(res[\"scores\"], res[\"labels\"])\n",
    "# \tsampled_batch_preds[res[\"model_name\"]].append(samples)\n",
    "# \tlow5, high5 = np.percentile(samples, P5), np.percentile(samples, 100 - P5)\n",
    "# \tlow1, high1 = np.percentile(samples, P1), np.percentile(samples, 100 - P1)\n",
    "# \troc_auc = sklearn.metrics.roc_auc_score(res[\"labels\"], res[\"scores\"])\n",
    "# \tif \"SummaC\" in res[\"model_name\"] or res[\"model_name\"] == strongest_baseline[res[\"dataset_name\"]]:\n",
    "# \t\tprint(res[\"dataset_name\"].ljust(15), res[\"model_name\"].ljust(20),\n",
    "# \t\t\t  \" - %.3f (%.3f - %.3f) (%.3f - %.3f)\" % (roc_auc, low5, high5, low1, high1))\n",
    "# \t\tif res[\"model_name\"] == strongest_baseline[res[\"dataset_name\"]]:\n",
    "# \t\t\tbl5, bh5, bl1, bh1 = low5, high5, low1, high1\n",
    "# \t\t\tprint(\"--------------\")\n",
    "# \t\telse:\n",
    "# \t\t\tif low5 >= bh5:\n",
    "# \t\t\t\tprint(\"Significant difference (p < 0.05)\")\n",
    "# \t\t\tif low1 >= bh1:\n",
    "# \t\t\t\tprint(\"Significant difference (p < 0.01)\")\n",
    "\n",
    "# print(\"==========================\")\n",
    "# print(\"==========================\")\n",
    "# print(\"==========================\")\n",
    "\n",
    "# baseline = np.mean(np.array(sampled_batch_preds[\"QuestEval\"]), axis=0)\n",
    "# summaczs = np.mean(np.array(sampled_batch_preds[\"SummaC-ZS-VITC-L\"]), axis=0)\n",
    "# summacconv = np.mean(np.array(sampled_batch_preds[\"SummaC-Histo-VITC-L\"]), axis=0)\n",
    "\n",
    "# for model in [\"QuestEval\", \"SummaC-ZS-VITC-L\", \"SummaC-Histo-VITC-L\"]:\n",
    "# \tsamples = np.mean(np.array(sampled_batch_preds[model]), axis=0)\n",
    "# \tlow5, high5 = np.percentile(samples, P5), np.percentile(samples, 100 - P5)\n",
    "# \tlow1, high1 = np.percentile(samples, P1), np.percentile(samples, 100 - P1)\n",
    "\n",
    "# \tprint(\"OVERALL\".ljust(15), model.ljust(20), \" - (%.3f - %.3f) (%.3f - %.3f)\" % (low5, high5, low1, high1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1625710190289,
   "trusted": true
  },
  "interpreter": {
   "hash": "c723e9d1a9662a11de23e7914e75631adbad784f516bc181f2c98a6790ee4bb2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
