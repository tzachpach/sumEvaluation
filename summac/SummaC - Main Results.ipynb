{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:49:21.106759600Z",
     "start_time": "2023-08-01T16:49:21.098253300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/AICOE/aviv/osint/sumVeracity/summac/summac\n"
     ]
    }
   ],
   "source": [
    "%cd summac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:49:28.497112Z",
     "start_time": "2023-08-01T16:49:21.405504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "from summac.benchmark import SummaCBenchmark\n",
    "# import summac.run_baseline as utils_summac_benchmark\n",
    "# import random\n",
    "import transformers\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "transformers.utils.logging.set_verbosity(transformers.logging.ERROR)\n",
    "print(transformers.utils.logging.get_verbosity())\n",
    "\n",
    "benchmark = SummaCBenchmark(benchmark_folder=\"data/summac_benchmark/\", cut=\"test\")\n",
    "# benchmark.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 2: Main Table of Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:49:30.057846500Z",
     "start_time": "2023-08-01T16:49:28.501111100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/elisheva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sklearn, torch, numpy as np, json, os, tqdm, pandas as pd, nltk, seaborn as sns\n",
    "from summac.model_guardrails import NERInaccuracyPenalty\n",
    "from summac.model_summac import SummaCConv, SummaCZS\n",
    "from summac.model_baseline import BaselineScorer\n",
    "# from model_entailment import EntailmentScorer\n",
    "# from model_classifier import Classifier\n",
    "from summac.utils_scoring import ScorerWrapper\n",
    "\n",
    "use_cache = True\n",
    "scorers = [\n",
    "# \t{\"name\": \"NER\", \"model\": NERInaccuracyPenalty(), \"only_doc\": True, \"sign\": 1},\n",
    "\t#     {\"name\": \"MNLI\", \"model\": EntailmentScorer(model_card=\"roberta-large-mnli\", contradiction_idx=0), \"sign\": 1},\n",
    "\t# {\"name\": \"FactCC-CLS\", \"model\": Classifier(model_card=\"roberta-base\", score_class=1, model_file=\"/home/phillab/models/cls_roberta-base_factcc_first_0_f1_0.4766.bin\"), \"sign\": 1, \"only_doc\": True},\n",
    "# \t{\"name\": \"DAE\", \"model\": BaselineScorer(model=\"dae\"), \"only_doc\": True, \"sign\": 1},\n",
    "# \t{\"name\": \"FEQA\", \"model\": BaselineScorer(model=\"feqa\"), \"only_doc\": True, \"sign\": 1},\n",
    "# \t{\"name\": \"QuestEval\", \"model\": BaselineScorer(model=\"questeval\"), \"only_doc\": True, \"sign\": 1},\n",
    "\t{\"name\": \"SummaC-ZS-VITC-L\",\n",
    "\t \"model\": SummaCZS(nli_granularity=\"sentence\", sts_granularity=\"sentence\", mlm_granularity=\"sentence\", model_name=\"vitc\", imager_load_cache=use_cache, use_nli=True, use_sts=True, use_mlm=True, device=\"cuda:13\"), \"sign\": 1,\n",
    "\t \"only_doc\": True},\n",
    "\t# {\"name\": \"SummaC-Histo-VITC-L\", \"model\": SummaCConv(models=[\"vitc\"], granularity=\"sentence\", start_file=\"/home/phillab/models/summac/vitc_sentence_percentile_e_bacc0.744.bin\", bins=\"percentile\", imager_load_cache=use_cache, device=\"cpu\"), \"sign\": 1, \"only_doc\": True},\n",
    "\n",
    "    # {\"name\": \"SummaC-ZS-VITC-L-NEW\",\n",
    "\t#  \"model\": SummaCZS(granularity=\"sentence\", model_name=\"vitc\", imager_load_cache=use_cache, device=\"cuda:13\"), \"sign\": 1,\n",
    "\t#  \"only_doc\": True},\n",
    "    # {\"name\": \"SummaC-ZS-VITC-L-para\",\n",
    "\t#  \"model\": SummaCZS(granularity=\"paragraph\", model_name=\"vitc\", imager_load_cache=use_cache, use_sts=False, device=\"cuda:13\"), \"sign\": 1},\n",
    "    # {\"name\": \"SummaC-ZS-VITC-L-NEW-para\",\n",
    "\t#  \"model\": SummaCZS(granularity=\"paragraph\", model_name=\"vitc\", imager_load_cache=use_cache, device=\"cuda:13\"), \"sign\": 1}\n",
    "]\n",
    "\n",
    "scorer_doc = ScorerWrapper(scorers, scoring_method=\"sum\", max_batch_size=50, use_caching=True)\n",
    "scorer_para = ScorerWrapper([s for s in scorers if \"only_doc\" not in s], scoring_method=\"sum\", max_batch_size=50,\n",
    "\t\t\t\t\t\t\tuse_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-08-01T16:49:53.191155Z",
     "start_time": "2023-08-01T16:49:30.058350100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= factcc ========\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c87d486b4d72490aba18952f6c95ea3f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f91e1dbfa0e4b6b84958dd2ebb037b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from summac.utils_scorer import compute_paragraph_level\n",
    "from summac.run_baseline import compute_doc_level\n",
    "\n",
    "results = []\n",
    "for dataset in benchmark.datasets:\n",
    "\tif dataset[\"name\"] != \"factcc\":\n",
    "\t\tcontinue\n",
    "\tprint(\"======= %s ========\" % (dataset[\"name\"]))\n",
    "\tdatas = dataset[\"dataset\"][:20]\n",
    "\tcompute_doc_level(datas, scorer_doc)\n",
    "\tcompute_paragraph_level(scorer_para, datas)\n",
    "\n",
    "\tlabels = [d[\"label\"] for d in datas]\n",
    "\tpred_labels = [k for k in datas[0].keys() if \"pred_\" in k]\n",
    "\tfor pred_label in pred_labels:\n",
    "\t\tmodel_name, input_type = pred_label.replace(\"pred_\", \"\").split(\"|\")\n",
    "\t\tpreds = [d[pred_label] for d in datas]\n",
    "\t\tscores = [d[pred_label.replace(\"pred_\", \"\")] for d in datas]\n",
    "\t\tbalanced_acc = sklearn.metrics.balanced_accuracy_score(labels, preds)\n",
    "\t\troc_auc = sklearn.metrics.roc_auc_score(labels, scores)\n",
    "\n",
    "\t\tresults.append({\"model_name\": model_name, \"dataset_name\": dataset[\"name\"],\n",
    "\t\t\t\t\t\t\"input\": input_type, \"%s_bacc\" % (dataset[\"name\"]): balanced_acc,\n",
    "\t\t\t\t\t\t\"%s_roc_auc\" % (dataset[\"name\"]): roc_auc,\n",
    "\t\t\t\t\t\t\"labels\": labels, \"preds\": preds, \"scores\": scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'model_name': 'SummaC-ZS-VITC-L',\n  'dataset_name': 'factcc',\n  'input': 'doc',\n  'factcc_bacc': 0.84375,\n  'factcc_roc_auc': 0.90625,\n  'labels': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1],\n  'preds': [1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1],\n  'scores': [2.3758292198181152,\n   1.5491322070360183,\n   1.2231752618349023,\n   1.6722859978675841,\n   1.86775302445447,\n   1.6837735367672784,\n   1.696451097726822,\n   1.3572572556634743,\n   1.410097321909335,\n   1.5498905062675477,\n   1.2654736333272674,\n   1.512182655079024,\n   0.47850164620294455,\n   1.1098865903913975,\n   1.988972532749176,\n   1.4693138011627727,\n   1.598238244652748,\n   1.394063325598836,\n   1.4898820221424103,\n   1.7413866966962814]},\n {'model_name': 'total',\n  'dataset_name': 'factcc',\n  'input': 'doc',\n  'factcc_bacc': 0.84375,\n  'factcc_roc_auc': 0.90625,\n  'labels': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1],\n  'preds': [1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1],\n  'scores': [2.3758292198181152,\n   1.5491322070360183,\n   1.2231752618349023,\n   1.6722859978675841,\n   1.86775302445447,\n   1.6837735367672784,\n   1.696451097726822,\n   1.3572572556634743,\n   1.410097321909335,\n   1.5498905062675477,\n   1.2654736333272674,\n   1.512182655079024,\n   0.47850164620294455,\n   1.1098865903913975,\n   1.988972532749176,\n   1.4693138011627727,\n   1.598238244652748,\n   1.394063325598836,\n   1.4898820221424103,\n   1.7413866966962814]}]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T16:49:53.244774600Z",
     "start_time": "2023-08-01T16:49:53.183645Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed for caching\n",
    "# for scorer in scorers:\n",
    "# \tif \"SummaC\" in scorer[\"name\"]:\n",
    "# \t\tscorer[\"model\"].save_imager_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results.to_parquet('results.parquet.gzip', compression='gzip')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7f81e80c11c0>",
      "text/html": "<style type=\"text/css\">\n#T_c198a_row0_col0, #T_c198a_row0_col1 {\n  font-weight: bold;\n  background-color: #ebf3eb;\n  color: #000000;\n}\n</style>\n<table id=\"T_c198a\">\n  <caption>Balanced Accuracy</caption>\n  <thead>\n    <tr>\n      <th class=\"blank\" >&nbsp;</th>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_c198a_level0_col0\" class=\"col_heading level0 col0\" >factcc</th>\n      <th id=\"T_c198a_level0_col1\" class=\"col_heading level0 col1\" >overall</th>\n    </tr>\n    <tr>\n      <th class=\"index_name level0\" >model_name</th>\n      <th class=\"index_name level1\" >input</th>\n      <th class=\"blank col0\" >&nbsp;</th>\n      <th class=\"blank col1\" >&nbsp;</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_c198a_level0_row0\" class=\"row_heading level0 row0\" >SummaC-ZS-VITC-L</th>\n      <th id=\"T_c198a_level1_row0\" class=\"row_heading level1 row0\" >doc</th>\n      <td id=\"T_c198a_row0_col0\" class=\"data row0 col0\" >0.843750</td>\n      <td id=\"T_c198a_row0_col1\" class=\"data row0 col1\" >0.843750</td>\n    </tr>\n  </tbody>\n</table>\n"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "\n",
    "\n",
    "def highlight_max(data):\n",
    "\tis_max = data == data.max()\n",
    "\treturn ['font-weight: bold' if v else '' for v in is_max]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df = df.groupby([\"model_name\", \"input\"]).agg({\"%s_bacc\" % (d): \"mean\" for d in [\"factcc\"]})\n",
    "df.rename(columns={k: k.replace(\"_bacc\", \"\") for k in df.keys()}, inplace=True)\n",
    "df.drop(\"total\", inplace=True)\n",
    "# df[\"overall\"] = (df[\"factcc\"] + df[\"frank\"] + df[\"polytope\"] + df[\"cogensumm\"] + df[\"summeval\"] + df[\"xsumfaith\"]) / (\n",
    "# \t6.0)\n",
    "\n",
    "df[\"overall\"] = df[\"factcc\"]\n",
    "\n",
    "df.style.apply(highlight_max).background_gradient(cmap=cm, high=1.0, low=0.0).set_caption(\n",
    "\t\"Balanced Accuracy\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T16:52:48.670847800Z",
     "start_time": "2023-08-01T16:52:48.649824Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T16:52:05.722953500Z",
     "start_time": "2023-08-01T16:52:05.309591600Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['frank_bacc', 'polytope_bacc', 'summeval_bacc', 'xsumfaith_bacc'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 10\u001B[0m\n\u001B[1;32m      6\u001B[0m \t\u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfont-weight: bold\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m v \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m is_max]\n\u001B[1;32m      9\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(results)\n\u001B[0;32m---> 10\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroupby\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel_name\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magg\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m%s\u001B[39;49;00m\u001B[38;5;124;43m_bacc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m%\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43md\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmean\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mxsumfaith\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpolytope\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfactcc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msummeval\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrank\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m df\u001B[38;5;241m.\u001B[39mrename(columns\u001B[38;5;241m=\u001B[39m{k: k\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_bacc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mkeys()}, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     12\u001B[0m df\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal\u001B[39m\u001B[38;5;124m\"\u001B[39m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/raid/AICOE/aviv/envs/lib/python3.8/site-packages/pandas/core/groupby/generic.py:1269\u001B[0m, in \u001B[0;36mDataFrameGroupBy.aggregate\u001B[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1266\u001B[0m func \u001B[38;5;241m=\u001B[39m maybe_mangle_lambdas(func)\n\u001B[1;32m   1268\u001B[0m op \u001B[38;5;241m=\u001B[39m GroupByApply(\u001B[38;5;28mself\u001B[39m, func, args, kwargs)\n\u001B[0;32m-> 1269\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magg\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1270\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_dict_like(func) \u001B[38;5;129;01mand\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1271\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m/raid/AICOE/aviv/envs/lib/python3.8/site-packages/pandas/core/apply.py:163\u001B[0m, in \u001B[0;36mApply.agg\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_dict_like(arg):\n\u001B[0;32m--> 163\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magg_dict_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_list_like(arg):\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;66;03m# we require a list, but not a 'str'\u001B[39;00m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magg_list_like()\n",
      "File \u001B[0;32m/raid/AICOE/aviv/envs/lib/python3.8/site-packages/pandas/core/apply.py:403\u001B[0m, in \u001B[0;36mApply.agg_dict_like\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    400\u001B[0m     selected_obj \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m_selected_obj\n\u001B[1;32m    401\u001B[0m     selection \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m_selection\n\u001B[0;32m--> 403\u001B[0m arg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalize_dictlike_arg\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43magg\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mselected_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    405\u001B[0m is_groupby \u001B[38;5;241m=\u001B[39m \u001B[38;5;28misinstance\u001B[39m(obj, (DataFrameGroupBy, SeriesGroupBy))\n\u001B[1;32m    406\u001B[0m context_manager: ContextManager\n",
      "File \u001B[0;32m/raid/AICOE/aviv/envs/lib/python3.8/site-packages/pandas/core/apply.py:535\u001B[0m, in \u001B[0;36mApply.normalize_dictlike_arg\u001B[0;34m(self, how, obj, func)\u001B[0m\n\u001B[1;32m    533\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(cols) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    534\u001B[0m         cols_sorted \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(safe_sort(\u001B[38;5;28mlist\u001B[39m(cols)))\n\u001B[0;32m--> 535\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn(s) \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcols_sorted\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m do not exist\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    537\u001B[0m aggregator_types \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mdict\u001B[39m)\n\u001B[1;32m    539\u001B[0m \u001B[38;5;66;03m# if we have a dict of any non-scalars\u001B[39;00m\n\u001B[1;32m    540\u001B[0m \u001B[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001B[39;00m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;66;03m# be list-likes\u001B[39;00m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001B[39;00m\n",
      "\u001B[0;31mKeyError\u001B[0m: \"Column(s) ['frank_bacc', 'polytope_bacc', 'summeval_bacc', 'xsumfaith_bacc'] do not exist\""
     ]
    }
   ],
   "source": [
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "\n",
    "\n",
    "def highlight_max(data):\n",
    "\tis_max = data == data.max()\n",
    "\treturn ['font-weight: bold' if v else '' for v in is_max]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df = df.groupby([\"model_name\", \"input\"]).agg({\"%s_bacc\" % (d): \"mean\" for d in [\"xsumfaith\", \"polytope\", \"factcc\", \"summeval\", \"frank\"]})\n",
    "df.rename(columns={k: k.replace(\"_bacc\", \"\") for k in df.keys()}, inplace=True)\n",
    "df.drop(\"total\", inplace=True)\n",
    "# df[\"overall\"] = (df[\"factcc\"] + df[\"frank\"] + df[\"polytope\"] + df[\"cogensumm\"] + df[\"summeval\"] + df[\"xsumfaith\"]) / (\n",
    "# \t6.0)\n",
    "\n",
    "df[\"overall\"] = (df[\"factcc\"] + df[\"frank\"] + df[\"polytope\"] + df[\"summeval\"] + df[\"xsumfaith\"]) / (\n",
    "\t5.0)\n",
    "\n",
    "df.style.apply(highlight_max).background_gradient(cmap=cm, high=1.0, low=0.0).set_precision(3).set_caption(\n",
    "\t\"Balanced Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # Analysis with confidence interval\n",
    "# strongest_baseline = {\"cogensumm\": \"DAE\", \"xsumfaith\": \"NER\", \"polytope\": \"QuestEval\", \"factcc\": \"DAE\",\n",
    "# \t\t\t\t\t  \"summeval\": \"QuestEval\", \"frank\": \"QuestEval\"}\n",
    "\n",
    "# P5 = 5 / 2  # Correction due to the fact that we are running 2 tests with the same data\n",
    "# P1 = 1 / 2  # Correction due to the fact that we are running 2 tests with the same data\n",
    "\n",
    "\n",
    "# def resample_balanced_acc(preds, labels, n_samples=100, sample_ratio=0.7):\n",
    "# \tN = len(preds)\n",
    "# \tidxs = list(range(N))\n",
    "# \tN_batch = int(sample_ratio * N)\n",
    "\n",
    "# \tbal_accs = []\n",
    "# \tfor _ in range(n_samples):\n",
    "# \t\trandom.shuffle(idxs)\n",
    "# \t\tbatch_preds = [preds[i] for i in idxs[:N_batch]]\n",
    "# \t\tbatch_labels = [labels[i] for i in idxs[:N_batch]]\n",
    "\n",
    "# \t\tbal_accs.append(sklearn.metrics.balanced_accuracy_score(batch_labels, batch_preds))\n",
    "# \treturn bal_accs\n",
    "\n",
    "\n",
    "# print(\"DATASET NAME\".ljust(15), \"MODEL NAME\".ljust(20))\n",
    "\n",
    "# sampled_batch_preds = {res[\"model_name\"]: [] for res in results}\n",
    "# for res in results:\n",
    "# \tif res[\"model_name\"] == \"total\":\n",
    "# \t\tprint(\"==================================================\")\n",
    "# \t\tcontinue\n",
    "\n",
    "# \tsamples = resample_balanced_acc(res[\"preds\"], res[\"labels\"])\n",
    "# \tsampled_batch_preds[res[\"model_name\"]].append(samples)\n",
    "# \tlow5, high5 = np.percentile(samples, P5), np.percentile(samples, 100 - P5)\n",
    "# \tlow1, high1 = np.percentile(samples, P1), np.percentile(samples, 100 - P1)\n",
    "# \tbacc = sklearn.metrics.balanced_accuracy_score(res[\"labels\"], res[\"preds\"])\n",
    "# \tif \"SummaC\" in res[\"model_name\"] or res[\"model_name\"] == strongest_baseline[res[\"dataset_name\"]]:\n",
    "\n",
    "# \t\tprint(res[\"dataset_name\"].ljust(15), res[\"model_name\"].ljust(20),\n",
    "# \t\t\t  \" - %.3f (%.3f - %.3f) (%.3f - %.3f)\" % (bacc, low5, high5, low1, high1))\n",
    "# \t\tif res[\"model_name\"] == strongest_baseline[res[\"dataset_name\"]]:\n",
    "# \t\t\tbl5, bh5, bl1, bh1 = low5, high5, low1, high1\n",
    "# \t\t\tprint(\"--------------\")\n",
    "# \t\telse:\n",
    "# \t\t\tif low5 >= bh5:\n",
    "# \t\t\t\tprint(\"Significant difference (p < 0.05)\")\n",
    "# \t\t\tif low1 >= bh1:\n",
    "# \t\t\t\tprint(\"Significant difference (p < 0.01)\")\n",
    "\n",
    "# print(\"==========================\")\n",
    "# print(\"==========================\")\n",
    "# print(\"==========================\")\n",
    "\n",
    "# # baseline = np.mean(np.array(sampled_batch_preds[\"QuestEval\"]), axis=0)\n",
    "# summaczs = np.mean(np.array(sampled_batch_preds[\"SummaC-ZS-VITC-L\"]), axis=0)\n",
    "# summacconv = np.mean(np.array(sampled_batch_preds[\"SummaC-Histo-VITC-L\"]), axis=0)\n",
    "\n",
    "# for model in [\"QuestEval\", \"SummaC-ZS-VITC-L\", \"SummaC-Histo-VITC-L\"]:\n",
    "# \tsamples = np.mean(np.array(sampled_batch_preds[model]), axis=0)\n",
    "# \tlow5, high5 = np.percentile(samples, P5), np.percentile(samples, 100 - P5)\n",
    "# \tlow1, high1 = np.percentile(samples, P1), np.percentile(samples, 100 - P1)\n",
    "\n",
    "# \tprint(\"OVERALL\".ljust(15), model.ljust(20), \" - (%.3f - %.3f) (%.3f - %.3f)\" % (low5, high5, low1, high1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df = df.groupby([\"model_name\", \"input\"]).agg({\"%s_roc_auc\" % (d): \"mean\" for d in [\"xsumfaith\", \"polytope\", \"factcc\", \"summeval\", \"frank\"]})\n",
    "df.rename(columns={k: k.replace(\"_roc_auc\", \"\") for k in df.keys()}, inplace=True)\n",
    "df.drop(\"total\", inplace=True)\n",
    "# df[\"overall\"] = (df[\"factcc\"] + df[\"frank\"] + df[\"polytope\"] + df[\"cogensumm\"] + df[\"summeval\"] + df[\"xsumfaith\"]) / (\n",
    "# \t6.0)\n",
    "\n",
    "df[\"overall\"] = (df[\"factcc\"] + df[\"frank\"] + df[\"polytope\"] + df[\"summeval\"] + df[\"xsumfaith\"]) / (\n",
    "\t5.0)\n",
    "\n",
    "df.style.apply(highlight_max).background_gradient(cmap=cm, high=1.0, low=0.0).set_precision(3).set_caption(\"ROC AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analysis with confidence interval\n",
    "# strongest_baseline = {\"cogensumm\": \"DAE\", \"xsumfaith\": \"QuestEval\", \"polytope\": \"QuestEval\", \"factcc\": \"DAE\",\n",
    "# \t\t\t\t\t  \"summeval\": \"QuestEval\", \"frank\": \"QuestEval\"}\n",
    "\n",
    "# P5 = 5 / 2  # Correction due to the fact that we are running 2 tests with the same data\n",
    "# P1 = 1 / 2  # Correction due to the fact that we are running 2 tests with the same data\n",
    "\n",
    "\n",
    "# def resample_roc_auc(scores, labels, n_samples=100, sample_ratio=0.7):\n",
    "# \tN = len(scores)\n",
    "# \tidxs = list(range(N))\n",
    "# \tN_batch = int(sample_ratio * N)\n",
    "\n",
    "# \troc_aucs = []\n",
    "# \tfor _ in range(n_samples):\n",
    "# \t\trandom.shuffle(idxs)\n",
    "# \t\tbatch_scores = [scores[i] for i in idxs[:N_batch]]\n",
    "# \t\tbatch_labels = [labels[i] for i in idxs[:N_batch]]\n",
    "# \t\troc_aucs.append(sklearn.metrics.roc_auc_score(batch_labels, batch_scores))\n",
    "# \treturn roc_aucs\n",
    "\n",
    "\n",
    "# sampled_batch_preds = {res[\"model_name\"]: [] for res in results}\n",
    "# print(\"DATASET NAME\".ljust(15), \"MODEL NAME\".ljust(20))\n",
    "# for res in results:\n",
    "# \tif res[\"model_name\"] == \"total\":\n",
    "# \t\tprint(\"==================================================\")\n",
    "# \t\tcontinue\n",
    "# \tsamples = resample_roc_auc(res[\"scores\"], res[\"labels\"])\n",
    "# \tsampled_batch_preds[res[\"model_name\"]].append(samples)\n",
    "# \tlow5, high5 = np.percentile(samples, P5), np.percentile(samples, 100 - P5)\n",
    "# \tlow1, high1 = np.percentile(samples, P1), np.percentile(samples, 100 - P1)\n",
    "# \troc_auc = sklearn.metrics.roc_auc_score(res[\"labels\"], res[\"scores\"])\n",
    "# \tif \"SummaC\" in res[\"model_name\"] or res[\"model_name\"] == strongest_baseline[res[\"dataset_name\"]]:\n",
    "# \t\tprint(res[\"dataset_name\"].ljust(15), res[\"model_name\"].ljust(20),\n",
    "# \t\t\t  \" - %.3f (%.3f - %.3f) (%.3f - %.3f)\" % (roc_auc, low5, high5, low1, high1))\n",
    "# \t\tif res[\"model_name\"] == strongest_baseline[res[\"dataset_name\"]]:\n",
    "# \t\t\tbl5, bh5, bl1, bh1 = low5, high5, low1, high1\n",
    "# \t\t\tprint(\"--------------\")\n",
    "# \t\telse:\n",
    "# \t\t\tif low5 >= bh5:\n",
    "# \t\t\t\tprint(\"Significant difference (p < 0.05)\")\n",
    "# \t\t\tif low1 >= bh1:\n",
    "# \t\t\t\tprint(\"Significant difference (p < 0.01)\")\n",
    "\n",
    "# print(\"==========================\")\n",
    "# print(\"==========================\")\n",
    "# print(\"==========================\")\n",
    "\n",
    "# baseline = np.mean(np.array(sampled_batch_preds[\"QuestEval\"]), axis=0)\n",
    "# summaczs = np.mean(np.array(sampled_batch_preds[\"SummaC-ZS-VITC-L\"]), axis=0)\n",
    "# summacconv = np.mean(np.array(sampled_batch_preds[\"SummaC-Histo-VITC-L\"]), axis=0)\n",
    "\n",
    "# for model in [\"QuestEval\", \"SummaC-ZS-VITC-L\", \"SummaC-Histo-VITC-L\"]:\n",
    "# \tsamples = np.mean(np.array(sampled_batch_preds[model]), axis=0)\n",
    "# \tlow5, high5 = np.percentile(samples, P5), np.percentile(samples, 100 - P5)\n",
    "# \tlow1, high1 = np.percentile(samples, P1), np.percentile(samples, 100 - P1)\n",
    "\n",
    "# \tprint(\"OVERALL\".ljust(15), model.ljust(20), \" - (%.3f - %.3f) (%.3f - %.3f)\" % (low5, high5, low1, high1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T06:09:36.891509800Z",
     "start_time": "2023-08-01T06:09:36.883512300Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1625710190289,
   "trusted": true
  },
  "interpreter": {
   "hash": "c723e9d1a9662a11de23e7914e75631adbad784f516bc181f2c98a6790ee4bb2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
