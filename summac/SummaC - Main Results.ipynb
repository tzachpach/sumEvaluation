{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\msc_work\\sumVeracity\\summac\\summac\n"
     ]
    }
   ],
   "source": [
    "%cd summac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summac.benchmark import SummaCBenchmark\n",
    "# import summac.run_baseline as utils_summac_benchmark\n",
    "# import random\n",
    "import transformers\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "benchmark = SummaCBenchmark(benchmark_folder=\"data/summac_benchmark/\", cut=\"test\")\n",
    "# benchmark.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 2: Main Table of Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aviv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sklearn, torch, numpy as np, json, os, tqdm, pandas as pd, nltk, seaborn as sns\n",
    "from summac.model_guardrails import NERInaccuracyPenalty\n",
    "from summac.model_summac import SummaCConv, SummaCZS\n",
    "from summac.model_baseline import BaselineScorer\n",
    "# from model_entailment import EntailmentScorer\n",
    "# from model_classifier import Classifier\n",
    "from summac.utils_scoring import ScorerWrapper\n",
    "\n",
    "use_cache = True\n",
    "scorers = [\n",
    "# \t{\"name\": \"NER\", \"model\": NERInaccuracyPenalty(), \"only_doc\": True, \"sign\": 1},\n",
    "\t#     {\"name\": \"MNLI\", \"model\": EntailmentScorer(model_card=\"roberta-large-mnli\", contradiction_idx=0), \"sign\": 1},\n",
    "\t# {\"name\": \"FactCC-CLS\", \"model\": Classifier(model_card=\"roberta-base\", score_class=1, model_file=\"/home/phillab/models/cls_roberta-base_factcc_first_0_f1_0.4766.bin\"), \"sign\": 1, \"only_doc\": True},\n",
    "# \t{\"name\": \"DAE\", \"model\": BaselineScorer(model=\"dae\"), \"only_doc\": True, \"sign\": 1},\n",
    "# \t{\"name\": \"FEQA\", \"model\": BaselineScorer(model=\"feqa\"), \"only_doc\": True, \"sign\": 1},\n",
    "# \t{\"name\": \"QuestEval\", \"model\": BaselineScorer(model=\"questeval\"), \"only_doc\": True, \"sign\": 1},\n",
    "\t{\"name\": \"SummaC-ZS-VITC-L\",\n",
    "\t \"model\": SummaCZS(granularity=\"sentence\", model_name=\"vitc\", imager_load_cache=use_cache, use_sts=False), \"sign\": 1,\n",
    "\t \"only_doc\": True},\n",
    "\t# {\"name\": \"SummaC-Histo-VITC-L\", \"model\": SummaCConv(models=[\"vitc\"], granularity=\"sentence\", start_file=\"/home/phillab/models/summac/vitc_sentence_percentile_e_bacc0.744.bin\", bins=\"percentile\", imager_load_cache=use_cache, device=\"cpu\"), \"sign\": 1, \"only_doc\": True},\n",
    "\n",
    "    {\"name\": \"SummaC-ZS-VITC-L-NEW\",\n",
    "\t \"model\": SummaCZS(granularity=\"sentence\", model_name=\"vitc\", imager_load_cache=use_cache), \"sign\": 1,\n",
    "\t \"only_doc\": True},\n",
    "    {\"name\": \"SummaC-ZS-VITC-L-para\",\n",
    "\t \"model\": SummaCZS(granularity=\"paragraph\", model_name=\"vitc\", imager_load_cache=use_cache, use_sts=False), \"sign\": 1},\n",
    "    {\"name\": \"SummaC-ZS-VITC-L-NEW-para\",\n",
    "\t \"model\": SummaCZS(granularity=\"paragraph\", model_name=\"vitc\", imager_load_cache=use_cache), \"sign\": 1}\n",
    "]\n",
    "\n",
    "scorer_doc = ScorerWrapper(scorers, scoring_method=\"sum\", max_batch_size=20, use_caching=True)\n",
    "scorer_para = ScorerWrapper([s for s in scorers if \"only_doc\" not in s], scoring_method=\"sum\", max_batch_size=20,\n",
    "\t\t\t\t\t\t\tuse_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= xsumfaith ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.36it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.22it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.31it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.28it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.23it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.40it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.48it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.49it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:00<00:00,  2.20it/s]\u001b[A\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.45it/s]\u001b[A\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.48it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.34it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.39it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.35it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:01,  2.32it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [00:00<00:00,  2.46it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [00:01<00:00,  2.49it/s]\u001b[A\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.52it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:00<00:00,  2.20it/s]\u001b[A\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.46it/s]\u001b[A\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.56it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.35it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:01<00:00,  2.40it/s]\u001b[A\n",
      " 80%|████████  | 4/5 [00:01<00:00,  2.40it/s]\u001b[A\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.78it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:01,  2.32it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.55it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "from summac.utils_scorer import compute_paragraph_level\n",
    "from summac.run_baseline import compute_doc_level\n",
    "\n",
    "results = []\n",
    "for dataset in benchmark.datasets:\n",
    "\tprint(\"======= %s ========\" % (dataset[\"name\"]))\n",
    "\tdatas = dataset[\"dataset\"][:50]\n",
    "\tcompute_doc_level(datas, scorer_doc)\n",
    "\tcompute_paragraph_level(scorer_para, datas)\n",
    "\n",
    "\tlabels = [d[\"label\"] for d in datas]\n",
    "\tpred_labels = [k for k in datas[0].keys() if \"pred_\" in k]\n",
    "\tfor pred_label in pred_labels:\n",
    "\t\tmodel_name, input_type = pred_label.replace(\"pred_\", \"\").split(\"|\")\n",
    "\t\tpreds = [d[pred_label] for d in datas]\n",
    "\t\tscores = [d[pred_label.replace(\"pred_\", \"\")] for d in datas]\n",
    "\t\tbalanced_acc = sklearn.metrics.balanced_accuracy_score(labels, preds)\n",
    "\t\troc_auc = sklearn.metrics.roc_auc_score(labels, scores)\n",
    "\n",
    "\t\tresults.append({\"model_name\": model_name, \"dataset_name\": dataset[\"name\"],\n",
    "\t\t\t\t\t\t\"input\": input_type, \"%s_bacc\" % (dataset[\"name\"]): balanced_acc,\n",
    "\t\t\t\t\t\t\"%s_roc_auc\" % (dataset[\"name\"]): roc_auc,\n",
    "\t\t\t\t\t\t\"labels\": labels, \"preds\": preds, \"scores\": scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed for caching\n",
    "# for scorer in scorers:\n",
    "# \tif \"SummaC\" in scorer[\"name\"]:\n",
    "# \t\tscorer[\"model\"].save_imager_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3a3d9_row0_col0, #T_3a3d9_row0_col1, #T_3a3d9_row0_col2, #T_3a3d9_row0_col3, #T_3a3d9_row0_col4, #T_3a3d9_row0_col5 {\n",
       "  font-weight: bold;\n",
       "  background-color: #ebf3eb;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3a3d9\">\n",
       "  <caption>Balanced Accuracy</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3a3d9_level0_col0\" class=\"col_heading level0 col0\" >xsumfaith</th>\n",
       "      <th id=\"T_3a3d9_level0_col1\" class=\"col_heading level0 col1\" >polytope</th>\n",
       "      <th id=\"T_3a3d9_level0_col2\" class=\"col_heading level0 col2\" >factcc</th>\n",
       "      <th id=\"T_3a3d9_level0_col3\" class=\"col_heading level0 col3\" >summeval</th>\n",
       "      <th id=\"T_3a3d9_level0_col4\" class=\"col_heading level0 col4\" >frank</th>\n",
       "      <th id=\"T_3a3d9_level0_col5\" class=\"col_heading level0 col5\" >overall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >model_name</th>\n",
       "      <th class=\"index_name level1\" >input</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3a3d9_level0_row0\" class=\"row_heading level0 row0\" >SummaC-ZS-VITC-L</th>\n",
       "      <th id=\"T_3a3d9_level1_row0\" class=\"row_heading level1 row0\" >doc</th>\n",
       "      <td id=\"T_3a3d9_row0_col0\" class=\"data row0 col0\" >0.733</td>\n",
       "      <td id=\"T_3a3d9_row0_col1\" class=\"data row0 col1\" >0.646</td>\n",
       "      <td id=\"T_3a3d9_row0_col2\" class=\"data row0 col2\" >0.692</td>\n",
       "      <td id=\"T_3a3d9_row0_col3\" class=\"data row0 col3\" >0.904</td>\n",
       "      <td id=\"T_3a3d9_row0_col4\" class=\"data row0 col4\" >0.641</td>\n",
       "      <td id=\"T_3a3d9_row0_col5\" class=\"data row0 col5\" >0.724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1977eb22250>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "\n",
    "\n",
    "def highlight_max(data):\n",
    "\tis_max = data == data.max()\n",
    "\treturn ['font-weight: bold' if v else '' for v in is_max]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df = df.groupby([\"model_name\", \"input\"]).agg({\"%s_bacc\" % (d): \"mean\" for d in [\"xsumfaith\", \"polytope\", \"factcc\", \"summeval\", \"frank\"]})\n",
    "df.rename(columns={k: k.replace(\"_bacc\", \"\") for k in df.keys()}, inplace=True)\n",
    "df.drop(\"total\", inplace=True)\n",
    "# df[\"overall\"] = (df[\"factcc\"] + df[\"frank\"] + df[\"polytope\"] + df[\"cogensumm\"] + df[\"summeval\"] + df[\"xsumfaith\"]) / (\n",
    "# \t6.0)\n",
    "\n",
    "df[\"overall\"] = (df[\"factcc\"] + df[\"frank\"] + df[\"polytope\"] + df[\"summeval\"] + df[\"xsumfaith\"]) / (\n",
    "\t5.0)\n",
    "\n",
    "df.style.apply(highlight_max).background_gradient(cmap=cm, high=1.0, low=0.0).set_precision(3).set_caption(\n",
    "\t\"Balanced Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # Analysis with confidence interval\n",
    "# strongest_baseline = {\"cogensumm\": \"DAE\", \"xsumfaith\": \"NER\", \"polytope\": \"QuestEval\", \"factcc\": \"DAE\",\n",
    "# \t\t\t\t\t  \"summeval\": \"QuestEval\", \"frank\": \"QuestEval\"}\n",
    "\n",
    "# P5 = 5 / 2  # Correction due to the fact that we are running 2 tests with the same data\n",
    "# P1 = 1 / 2  # Correction due to the fact that we are running 2 tests with the same data\n",
    "\n",
    "\n",
    "# def resample_balanced_acc(preds, labels, n_samples=100, sample_ratio=0.7):\n",
    "# \tN = len(preds)\n",
    "# \tidxs = list(range(N))\n",
    "# \tN_batch = int(sample_ratio * N)\n",
    "\n",
    "# \tbal_accs = []\n",
    "# \tfor _ in range(n_samples):\n",
    "# \t\trandom.shuffle(idxs)\n",
    "# \t\tbatch_preds = [preds[i] for i in idxs[:N_batch]]\n",
    "# \t\tbatch_labels = [labels[i] for i in idxs[:N_batch]]\n",
    "\n",
    "# \t\tbal_accs.append(sklearn.metrics.balanced_accuracy_score(batch_labels, batch_preds))\n",
    "# \treturn bal_accs\n",
    "\n",
    "\n",
    "# print(\"DATASET NAME\".ljust(15), \"MODEL NAME\".ljust(20))\n",
    "\n",
    "# sampled_batch_preds = {res[\"model_name\"]: [] for res in results}\n",
    "# for res in results:\n",
    "# \tif res[\"model_name\"] == \"total\":\n",
    "# \t\tprint(\"==================================================\")\n",
    "# \t\tcontinue\n",
    "\n",
    "# \tsamples = resample_balanced_acc(res[\"preds\"], res[\"labels\"])\n",
    "# \tsampled_batch_preds[res[\"model_name\"]].append(samples)\n",
    "# \tlow5, high5 = np.percentile(samples, P5), np.percentile(samples, 100 - P5)\n",
    "# \tlow1, high1 = np.percentile(samples, P1), np.percentile(samples, 100 - P1)\n",
    "# \tbacc = sklearn.metrics.balanced_accuracy_score(res[\"labels\"], res[\"preds\"])\n",
    "# \tif \"SummaC\" in res[\"model_name\"] or res[\"model_name\"] == strongest_baseline[res[\"dataset_name\"]]:\n",
    "\n",
    "# \t\tprint(res[\"dataset_name\"].ljust(15), res[\"model_name\"].ljust(20),\n",
    "# \t\t\t  \" - %.3f (%.3f - %.3f) (%.3f - %.3f)\" % (bacc, low5, high5, low1, high1))\n",
    "# \t\tif res[\"model_name\"] == strongest_baseline[res[\"dataset_name\"]]:\n",
    "# \t\t\tbl5, bh5, bl1, bh1 = low5, high5, low1, high1\n",
    "# \t\t\tprint(\"--------------\")\n",
    "# \t\telse:\n",
    "# \t\t\tif low5 >= bh5:\n",
    "# \t\t\t\tprint(\"Significant difference (p < 0.05)\")\n",
    "# \t\t\tif low1 >= bh1:\n",
    "# \t\t\t\tprint(\"Significant difference (p < 0.01)\")\n",
    "\n",
    "# print(\"==========================\")\n",
    "# print(\"==========================\")\n",
    "# print(\"==========================\")\n",
    "\n",
    "# # baseline = np.mean(np.array(sampled_batch_preds[\"QuestEval\"]), axis=0)\n",
    "# summaczs = np.mean(np.array(sampled_batch_preds[\"SummaC-ZS-VITC-L\"]), axis=0)\n",
    "# summacconv = np.mean(np.array(sampled_batch_preds[\"SummaC-Histo-VITC-L\"]), axis=0)\n",
    "\n",
    "# for model in [\"QuestEval\", \"SummaC-ZS-VITC-L\", \"SummaC-Histo-VITC-L\"]:\n",
    "# \tsamples = np.mean(np.array(sampled_batch_preds[model]), axis=0)\n",
    "# \tlow5, high5 = np.percentile(samples, P5), np.percentile(samples, 100 - P5)\n",
    "# \tlow1, high1 = np.percentile(samples, P1), np.percentile(samples, 100 - P1)\n",
    "\n",
    "# \tprint(\"OVERALL\".ljust(15), model.ljust(20), \" - (%.3f - %.3f) (%.3f - %.3f)\" % (low5, high5, low1, high1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d8425_row0_col0, #T_d8425_row0_col1, #T_d8425_row0_col2, #T_d8425_row0_col3, #T_d8425_row0_col4, #T_d8425_row0_col5 {\n",
       "  font-weight: bold;\n",
       "  background-color: #ebf3eb;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d8425\">\n",
       "  <caption>ROC AUC</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d8425_level0_col0\" class=\"col_heading level0 col0\" >xsumfaith</th>\n",
       "      <th id=\"T_d8425_level0_col1\" class=\"col_heading level0 col1\" >polytope</th>\n",
       "      <th id=\"T_d8425_level0_col2\" class=\"col_heading level0 col2\" >factcc</th>\n",
       "      <th id=\"T_d8425_level0_col3\" class=\"col_heading level0 col3\" >summeval</th>\n",
       "      <th id=\"T_d8425_level0_col4\" class=\"col_heading level0 col4\" >frank</th>\n",
       "      <th id=\"T_d8425_level0_col5\" class=\"col_heading level0 col5\" >overall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >model_name</th>\n",
       "      <th class=\"index_name level1\" >input</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d8425_level0_row0\" class=\"row_heading level0 row0\" >SummaC-ZS-VITC-L</th>\n",
       "      <th id=\"T_d8425_level1_row0\" class=\"row_heading level1 row0\" >doc</th>\n",
       "      <td id=\"T_d8425_row0_col0\" class=\"data row0 col0\" >0.716</td>\n",
       "      <td id=\"T_d8425_row0_col1\" class=\"data row0 col1\" >0.618</td>\n",
       "      <td id=\"T_d8425_row0_col2\" class=\"data row0 col2\" >0.656</td>\n",
       "      <td id=\"T_d8425_row0_col3\" class=\"data row0 col3\" >0.872</td>\n",
       "      <td id=\"T_d8425_row0_col4\" class=\"data row0 col4\" >0.614</td>\n",
       "      <td id=\"T_d8425_row0_col5\" class=\"data row0 col5\" >0.695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1977eb278e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df = df.groupby([\"model_name\", \"input\"]).agg({\"%s_roc_auc\" % (d): \"mean\" for d in [\"xsumfaith\", \"polytope\", \"factcc\", \"summeval\", \"frank\"]})\n",
    "df.rename(columns={k: k.replace(\"_roc_auc\", \"\") for k in df.keys()}, inplace=True)\n",
    "df.drop(\"total\", inplace=True)\n",
    "# df[\"overall\"] = (df[\"factcc\"] + df[\"frank\"] + df[\"polytope\"] + df[\"cogensumm\"] + df[\"summeval\"] + df[\"xsumfaith\"]) / (\n",
    "# \t6.0)\n",
    "\n",
    "df[\"overall\"] = (df[\"factcc\"] + df[\"frank\"] + df[\"polytope\"] + df[\"summeval\"] + df[\"xsumfaith\"]) / (\n",
    "\t5.0)\n",
    "\n",
    "df.style.apply(highlight_max).background_gradient(cmap=cm, high=1.0, low=0.0).set_precision(3).set_caption(\"ROC AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analysis with confidence interval\n",
    "# strongest_baseline = {\"cogensumm\": \"DAE\", \"xsumfaith\": \"QuestEval\", \"polytope\": \"QuestEval\", \"factcc\": \"DAE\",\n",
    "# \t\t\t\t\t  \"summeval\": \"QuestEval\", \"frank\": \"QuestEval\"}\n",
    "\n",
    "# P5 = 5 / 2  # Correction due to the fact that we are running 2 tests with the same data\n",
    "# P1 = 1 / 2  # Correction due to the fact that we are running 2 tests with the same data\n",
    "\n",
    "\n",
    "# def resample_roc_auc(scores, labels, n_samples=100, sample_ratio=0.7):\n",
    "# \tN = len(scores)\n",
    "# \tidxs = list(range(N))\n",
    "# \tN_batch = int(sample_ratio * N)\n",
    "\n",
    "# \troc_aucs = []\n",
    "# \tfor _ in range(n_samples):\n",
    "# \t\trandom.shuffle(idxs)\n",
    "# \t\tbatch_scores = [scores[i] for i in idxs[:N_batch]]\n",
    "# \t\tbatch_labels = [labels[i] for i in idxs[:N_batch]]\n",
    "# \t\troc_aucs.append(sklearn.metrics.roc_auc_score(batch_labels, batch_scores))\n",
    "# \treturn roc_aucs\n",
    "\n",
    "\n",
    "# sampled_batch_preds = {res[\"model_name\"]: [] for res in results}\n",
    "# print(\"DATASET NAME\".ljust(15), \"MODEL NAME\".ljust(20))\n",
    "# for res in results:\n",
    "# \tif res[\"model_name\"] == \"total\":\n",
    "# \t\tprint(\"==================================================\")\n",
    "# \t\tcontinue\n",
    "# \tsamples = resample_roc_auc(res[\"scores\"], res[\"labels\"])\n",
    "# \tsampled_batch_preds[res[\"model_name\"]].append(samples)\n",
    "# \tlow5, high5 = np.percentile(samples, P5), np.percentile(samples, 100 - P5)\n",
    "# \tlow1, high1 = np.percentile(samples, P1), np.percentile(samples, 100 - P1)\n",
    "# \troc_auc = sklearn.metrics.roc_auc_score(res[\"labels\"], res[\"scores\"])\n",
    "# \tif \"SummaC\" in res[\"model_name\"] or res[\"model_name\"] == strongest_baseline[res[\"dataset_name\"]]:\n",
    "# \t\tprint(res[\"dataset_name\"].ljust(15), res[\"model_name\"].ljust(20),\n",
    "# \t\t\t  \" - %.3f (%.3f - %.3f) (%.3f - %.3f)\" % (roc_auc, low5, high5, low1, high1))\n",
    "# \t\tif res[\"model_name\"] == strongest_baseline[res[\"dataset_name\"]]:\n",
    "# \t\t\tbl5, bh5, bl1, bh1 = low5, high5, low1, high1\n",
    "# \t\t\tprint(\"--------------\")\n",
    "# \t\telse:\n",
    "# \t\t\tif low5 >= bh5:\n",
    "# \t\t\t\tprint(\"Significant difference (p < 0.05)\")\n",
    "# \t\t\tif low1 >= bh1:\n",
    "# \t\t\t\tprint(\"Significant difference (p < 0.01)\")\n",
    "\n",
    "# print(\"==========================\")\n",
    "# print(\"==========================\")\n",
    "# print(\"==========================\")\n",
    "\n",
    "# baseline = np.mean(np.array(sampled_batch_preds[\"QuestEval\"]), axis=0)\n",
    "# summaczs = np.mean(np.array(sampled_batch_preds[\"SummaC-ZS-VITC-L\"]), axis=0)\n",
    "# summacconv = np.mean(np.array(sampled_batch_preds[\"SummaC-Histo-VITC-L\"]), axis=0)\n",
    "\n",
    "# for model in [\"QuestEval\", \"SummaC-ZS-VITC-L\", \"SummaC-Histo-VITC-L\"]:\n",
    "# \tsamples = np.mean(np.array(sampled_batch_preds[model]), axis=0)\n",
    "# \tlow5, high5 = np.percentile(samples, P5), np.percentile(samples, 100 - P5)\n",
    "# \tlow1, high1 = np.percentile(samples, P1), np.percentile(samples, 100 - P1)\n",
    "\n",
    "# \tprint(\"OVERALL\".ljust(15), model.ljust(20), \" - (%.3f - %.3f) (%.3f - %.3f)\" % (low5, high5, low1, high1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1625710190289,
   "trusted": true
  },
  "interpreter": {
   "hash": "c723e9d1a9662a11de23e7914e75631adbad784f516bc181f2c98a6790ee4bb2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
