{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-28T10:00:45.677657500Z",
     "start_time": "2023-07-28T10:00:45.661619100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\msc_work\\sumVeracity\\summac\\summac\n"
     ]
    }
   ],
   "source": [
    "%cd summac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summac.benchmark import SummaCBenchmark\n",
    "# import summac.run_baseline as utils_summac_benchmark\n",
    "# import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# benchmark = SummaCBenchmark(benchmark_folder=\"summac/data/summac_benchmark/\", cut=\"test\")\n",
    "# benchmark.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 2: Main Table of Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn, torch, numpy as np, json, os, tqdm, pandas as pd, nltk, seaborn as sns\n",
    "from summac.model_guardrails import NERInaccuracyPenalty\n",
    "from summac.model_summac import SummaCConv, SummaCZS\n",
    "from summac.model_baseline import BaselineScorer\n",
    "# from model_entailment import EntailmentScorer\n",
    "# from model_classifier import Classifier\n",
    "from summac.utils_scoring import ScorerWrapper\n",
    "\n",
    "use_cache = True\n",
    "scorers = [\n",
    "\t{\"name\": \"NER\", \"model\": NERInaccuracyPenalty(), \"only_doc\": True, \"sign\": 1},\n",
    "\t#     {\"name\": \"MNLI\", \"model\": EntailmentScorer(model_card=\"roberta-large-mnli\", contradiction_idx=0), \"sign\": 1},\n",
    "\t# {\"name\": \"FactCC-CLS\", \"model\": Classifier(model_card=\"roberta-base\", score_class=1, model_file=\"/home/phillab/models/cls_roberta-base_factcc_first_0_f1_0.4766.bin\"), \"sign\": 1, \"only_doc\": True},\n",
    "\t{\"name\": \"DAE\", \"model\": BaselineScorer(model=\"dae\"), \"only_doc\": True, \"sign\": 1},\n",
    "\t{\"name\": \"FEQA\", \"model\": BaselineScorer(model=\"feqa\"), \"only_doc\": True, \"sign\": 1},\n",
    "\t{\"name\": \"QuestEval\", \"model\": BaselineScorer(model=\"questeval\"), \"only_doc\": True, \"sign\": 1},\n",
    "\t{\"name\": \"SummaC-ZS-VITC-L\",\n",
    "\t \"model\": SummaCZS(granularity=\"sentence\", model_name=\"vitc\", imager_load_cache=use_cache), \"sign\": 1,\n",
    "\t \"only_doc\": True},\n",
    "\t# {\"name\": \"SummaC-Histo-VITC-L\", \"model\": SummaCConv(models=[\"vitc\"], granularity=\"sentence\", start_file=\"/home/phillab/models/summac/vitc_sentence_percentile_e_bacc0.744.bin\", bins=\"percentile\", imager_load_cache=use_cache, device=\"cpu\"), \"sign\": 1, \"only_doc\": True},\n",
    "]\n",
    "\n",
    "scorer_doc = ScorerWrapper(scorers, scoring_method=\"sum\", max_batch_size=20, use_caching=True)\n",
    "scorer_para = ScorerWrapper([s for s in scorers if \"only_doc\" not in s], scoring_method=\"sum\", max_batch_size=20,\n",
    "\t\t\t\t\t\t\tuse_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (C:/Users/Aviv/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b71012846d484286e9077074a3b6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a7b56407e941e89c31fe1abe97726f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0536c4caa5f04f009ce6893c8b753451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/9.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e794196f164d61ab44fa93a6e8b089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/15.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cnn_dailymail/3.0.0 (download: 558.32 MiB, generated: 1.28 GiB, post-processed: Unknown size, total: 1.82 GiB) to C:/Users/Aviv/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0332e013fcb54f68ab20767793c62016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138970a4c82b4ecfaade360bc1e96871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/159M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555990132f1c419d850c281564bbd518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/376M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8aa1f7d26b4053a362537e90eb18c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8961c8b93c1041ccb9523768ac6e4f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/661k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c79d777be14c7c9dd5431eb98d7fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/572k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b013e80badf4ceeb126c48ab890a8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from summac import run_baseline, utils_scorer\n",
    "\n",
    "results = []\n",
    "for dataset in benchmark.datasets:\n",
    "\tprint(\"======= %s ========\" % (dataset[\"name\"]))\n",
    "\tdatas = dataset[\"task\"]\n",
    "\trun_baseline.compute_doc_level(datas)\n",
    "\tutils_scorer.compute_paragraph_level(scorer_para, datas)\n",
    "\n",
    "\tlabels = [d[\"label\"] for d in datas]\n",
    "\tpred_labels = [k for k in datas[0].keys() if \"pred_\" in k]\n",
    "\tfor pred_label in pred_labels:\n",
    "\t\tmodel_name, input_type = pred_label.replace(\"pred_\", \"\").split(\"|\")\n",
    "\t\tpreds = [d[pred_label] for d in datas]\n",
    "\t\tscores = [d[pred_label.replace(\"pred_\", \"\")] for d in datas]\n",
    "\t\tbalanced_acc = sklearn.metrics.balanced_accuracy_score(labels, preds)\n",
    "\t\troc_auc = sklearn.metrics.roc_auc_score(labels, scores)\n",
    "\n",
    "\t\tresults.append({\"model_name\": model_name, \"dataset_name\": dataset[\"name\"],\n",
    "\t\t\t\t\t\t\"input\": input_type, \"%s_bacc\" % (dataset[\"name\"]): balanced_acc,\n",
    "\t\t\t\t\t\t\"%s_roc_auc\" % (dataset[\"name\"]): roc_auc,\n",
    "\t\t\t\t\t\t\"labels\": labels, \"preds\": preds, \"scores\": scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scorers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# If needed for caching\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scorer \u001b[38;5;129;01min\u001b[39;00m \u001b[43mscorers\u001b[49m:\n\u001b[0;32m      3\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummaC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m scorer[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m      4\u001b[0m \t\tscorer[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msave_imager_cache()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scorers' is not defined"
     ]
    }
   ],
   "source": [
    "# If needed for caching\n",
    "for scorer in scorers:\n",
    "\tif \"SummaC\" in scorer[\"name\"]:\n",
    "\t\tscorer[\"model\"].save_imager_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "\n",
    "\n",
    "def highlight_max(data):\n",
    "\tis_max = data == data.max()\n",
    "\treturn ['font-weight: bold' if v else '' for v in is_max]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df = df.groupby([\"model_name\", \"input\"]).agg({\"%s_bacc\" % (d): \"mean\" for d in benchmark.task_name_to_task})\n",
    "df.rename(columns={k: k.replace(\"_bacc\", \"\") for k in df.keys()}, inplace=True)\n",
    "df.drop(\"total\", inplace=True)\n",
    "df[\"overall\"] = (df[\"factcc\"] + df[\"frank\"] + df[\"polytope\"] + df[\"cogensumm\"] + df[\"summeval\"] + df[\"xsumfaith\"]) / (\n",
    "\t6.0)\n",
    "\n",
    "df.style.apply(highlight_max).background_gradient(cmap=cm, high=1.0, low=0.0).set_precision(3).set_caption(\n",
    "\t\"Balanced Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Analysis with confidence interval\n",
    "strongest_baseline = {\"cogensumm\": \"DAE\", \"xsumfaith\": \"NER\", \"polytope\": \"QuestEval\", \"factcc\": \"DAE\",\n",
    "\t\t\t\t\t  \"summeval\": \"QuestEval\", \"frank\": \"QuestEval\"}\n",
    "\n",
    "P5 = 5 / 2  # Correction due to the fact that we are running 2 tests with the same data\n",
    "P1 = 1 / 2  # Correction due to the fact that we are running 2 tests with the same data\n",
    "\n",
    "\n",
    "def resample_balanced_acc(preds, labels, n_samples=100, sample_ratio=0.7):\n",
    "\tN = len(preds)\n",
    "\tidxs = list(range(N))\n",
    "\tN_batch = int(sample_ratio * N)\n",
    "\n",
    "\tbal_accs = []\n",
    "\tfor _ in range(n_samples):\n",
    "\t\trandom.shuffle(idxs)\n",
    "\t\tbatch_preds = [preds[i] for i in idxs[:N_batch]]\n",
    "\t\tbatch_labels = [labels[i] for i in idxs[:N_batch]]\n",
    "\n",
    "\t\tbal_accs.append(sklearn.metrics.balanced_accuracy_score(batch_labels, batch_preds))\n",
    "\treturn bal_accs\n",
    "\n",
    "\n",
    "print(\"DATASET NAME\".ljust(15), \"MODEL NAME\".ljust(20))\n",
    "\n",
    "sampled_batch_preds = {res[\"model_name\"]: [] for res in results}\n",
    "for res in results:\n",
    "\tif res[\"model_name\"] == \"total\":\n",
    "\t\tprint(\"==================================================\")\n",
    "\t\tcontinue\n",
    "\n",
    "\tsamples = resample_balanced_acc(res[\"preds\"], res[\"labels\"])\n",
    "\tsampled_batch_preds[res[\"model_name\"]].append(samples)\n",
    "\tlow5, high5 = np.percentile(samples, P5), np.percentile(samples, 100 - P5)\n",
    "\tlow1, high1 = np.percentile(samples, P1), np.percentile(samples, 100 - P1)\n",
    "\tbacc = sklearn.metrics.balanced_accuracy_score(res[\"labels\"], res[\"preds\"])\n",
    "\tif \"SummaC\" in res[\"model_name\"] or res[\"model_name\"] == strongest_baseline[res[\"dataset_name\"]]:\n",
    "\n",
    "\t\tprint(res[\"dataset_name\"].ljust(15), res[\"model_name\"].ljust(20),\n",
    "\t\t\t  \" - %.3f (%.3f - %.3f) (%.3f - %.3f)\" % (bacc, low5, high5, low1, high1))\n",
    "\t\tif res[\"model_name\"] == strongest_baseline[res[\"dataset_name\"]]:\n",
    "\t\t\tbl5, bh5, bl1, bh1 = low5, high5, low1, high1\n",
    "\t\t\tprint(\"--------------\")\n",
    "\t\telse:\n",
    "\t\t\tif low5 >= bh5:\n",
    "\t\t\t\tprint(\"Significant difference (p < 0.05)\")\n",
    "\t\t\tif low1 >= bh1:\n",
    "\t\t\t\tprint(\"Significant difference (p < 0.01)\")\n",
    "\n",
    "print(\"==========================\")\n",
    "print(\"==========================\")\n",
    "print(\"==========================\")\n",
    "\n",
    "# baseline = np.mean(np.array(sampled_batch_preds[\"QuestEval\"]), axis=0)\n",
    "summaczs = np.mean(np.array(sampled_batch_preds[\"SummaC-ZS-VITC-L\"]), axis=0)\n",
    "summacconv = np.mean(np.array(sampled_batch_preds[\"SummaC-Histo-VITC-L\"]), axis=0)\n",
    "\n",
    "for model in [\"QuestEval\", \"SummaC-ZS-VITC-L\", \"SummaC-Histo-VITC-L\"]:\n",
    "\tsamples = np.mean(np.array(sampled_batch_preds[model]), axis=0)\n",
    "\tlow5, high5 = np.percentile(samples, P5), np.percentile(samples, 100 - P5)\n",
    "\tlow1, high1 = np.percentile(samples, P1), np.percentile(samples, 100 - P1)\n",
    "\n",
    "\tprint(\"OVERALL\".ljust(15), model.ljust(20), \" - (%.3f - %.3f) (%.3f - %.3f)\" % (low5, high5, low1, high1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df = df.groupby([\"model_name\", \"input\"]).agg({\"%s_roc_auc\" % (d): \"mean\" for d in benchmark.task_name_to_task})\n",
    "df.rename(columns={k: k.replace(\"_roc_auc\", \"\") for k in df.keys()}, inplace=True)\n",
    "df.drop(\"total\", inplace=True)\n",
    "df[\"overall\"] = (df[\"factcc\"] + df[\"frank\"] + df[\"polytope\"] + df[\"cogensumm\"] + df[\"summeval\"] + df[\"xsumfaith\"]) / (\n",
    "\t6.0)\n",
    "\n",
    "df.style.apply(highlight_max).background_gradient(cmap=cm, high=1.0, low=0.0).set_precision(3).set_caption(\"ROC AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis with confidence interval\n",
    "strongest_baseline = {\"cogensumm\": \"DAE\", \"xsumfaith\": \"QuestEval\", \"polytope\": \"QuestEval\", \"factcc\": \"DAE\",\n",
    "\t\t\t\t\t  \"summeval\": \"QuestEval\", \"frank\": \"QuestEval\"}\n",
    "\n",
    "P5 = 5 / 2  # Correction due to the fact that we are running 2 tests with the same data\n",
    "P1 = 1 / 2  # Correction due to the fact that we are running 2 tests with the same data\n",
    "\n",
    "\n",
    "def resample_roc_auc(scores, labels, n_samples=100, sample_ratio=0.7):\n",
    "\tN = len(scores)\n",
    "\tidxs = list(range(N))\n",
    "\tN_batch = int(sample_ratio * N)\n",
    "\n",
    "\troc_aucs = []\n",
    "\tfor _ in range(n_samples):\n",
    "\t\trandom.shuffle(idxs)\n",
    "\t\tbatch_scores = [scores[i] for i in idxs[:N_batch]]\n",
    "\t\tbatch_labels = [labels[i] for i in idxs[:N_batch]]\n",
    "\t\troc_aucs.append(sklearn.metrics.roc_auc_score(batch_labels, batch_scores))\n",
    "\treturn roc_aucs\n",
    "\n",
    "\n",
    "sampled_batch_preds = {res[\"model_name\"]: [] for res in results}\n",
    "print(\"DATASET NAME\".ljust(15), \"MODEL NAME\".ljust(20))\n",
    "for res in results:\n",
    "\tif res[\"model_name\"] == \"total\":\n",
    "\t\tprint(\"==================================================\")\n",
    "\t\tcontinue\n",
    "\tsamples = resample_roc_auc(res[\"scores\"], res[\"labels\"])\n",
    "\tsampled_batch_preds[res[\"model_name\"]].append(samples)\n",
    "\tlow5, high5 = np.percentile(samples, P5), np.percentile(samples, 100 - P5)\n",
    "\tlow1, high1 = np.percentile(samples, P1), np.percentile(samples, 100 - P1)\n",
    "\troc_auc = sklearn.metrics.roc_auc_score(res[\"labels\"], res[\"scores\"])\n",
    "\tif \"SummaC\" in res[\"model_name\"] or res[\"model_name\"] == strongest_baseline[res[\"dataset_name\"]]:\n",
    "\t\tprint(res[\"dataset_name\"].ljust(15), res[\"model_name\"].ljust(20),\n",
    "\t\t\t  \" - %.3f (%.3f - %.3f) (%.3f - %.3f)\" % (roc_auc, low5, high5, low1, high1))\n",
    "\t\tif res[\"model_name\"] == strongest_baseline[res[\"dataset_name\"]]:\n",
    "\t\t\tbl5, bh5, bl1, bh1 = low5, high5, low1, high1\n",
    "\t\t\tprint(\"--------------\")\n",
    "\t\telse:\n",
    "\t\t\tif low5 >= bh5:\n",
    "\t\t\t\tprint(\"Significant difference (p < 0.05)\")\n",
    "\t\t\tif low1 >= bh1:\n",
    "\t\t\t\tprint(\"Significant difference (p < 0.01)\")\n",
    "\n",
    "print(\"==========================\")\n",
    "print(\"==========================\")\n",
    "print(\"==========================\")\n",
    "\n",
    "baseline = np.mean(np.array(sampled_batch_preds[\"QuestEval\"]), axis=0)\n",
    "summaczs = np.mean(np.array(sampled_batch_preds[\"SummaC-ZS-VITC-L\"]), axis=0)\n",
    "summacconv = np.mean(np.array(sampled_batch_preds[\"SummaC-Histo-VITC-L\"]), axis=0)\n",
    "\n",
    "for model in [\"QuestEval\", \"SummaC-ZS-VITC-L\", \"SummaC-Histo-VITC-L\"]:\n",
    "\tsamples = np.mean(np.array(sampled_batch_preds[model]), axis=0)\n",
    "\tlow5, high5 = np.percentile(samples, P5), np.percentile(samples, 100 - P5)\n",
    "\tlow1, high1 = np.percentile(samples, P1), np.percentile(samples, 100 - P1)\n",
    "\n",
    "\tprint(\"OVERALL\".ljust(15), model.ljust(20), \" - (%.3f - %.3f) (%.3f - %.3f)\" % (low5, high5, low1, high1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1625710190289,
   "trusted": true
  },
  "interpreter": {
   "hash": "c723e9d1a9662a11de23e7914e75631adbad784f516bc181f2c98a6790ee4bb2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
